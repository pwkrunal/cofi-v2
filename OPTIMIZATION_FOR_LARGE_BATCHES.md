# Optimization Guide for Large Batches (10,000+ Files)

## Overview

For **10,000 files per batch**, the default configuration would generate **~168,000 database INSERT operations**. This guide shows how to optimize for this scale.

---

## üìä Database Load Comparison

### Default Configuration (Not Optimized):

| Component | INSERTs per Batch |
|-----------|-------------------|
| File Distribution | 21,002 |
| Denoise | 21,002 |
| IVR | 21,002 |
| LID | 21,002 |
| STT | 21,002 |
| LLM1 | 21,002 |
| LLM2 | 21,002 |
| Metadata + Triaging | 6 |
| **TOTAL** | **~168,000 INSERTs** üö® |

**Breakdown per stage:**
- 1 stage_start
- 10,000 file_start events
- 10,000 file_complete events
- 1,000 progress updates (every 10 files)
- 1 stage_complete

### Optimized Configuration (Recommended):

| Component | INSERTs per Batch |
|-----------|-------------------|
| File Distribution | 11,002 |
| Denoise | 11,002 |
| IVR | 11,002 |
| LID | 11,002 |
| STT | 11,002 |
| LLM1 | 11,002 |
| LLM2 | 11,002 |
| Metadata + Triaging | 6 |
| **TOTAL** | **~88,000 INSERTs** ‚úÖ |

**Savings: 80,000 fewer INSERTs (48% reduction!)** üéâ

**Breakdown per stage (optimized):**
- 1 stage_start
- ~~10,000 file_start events~~ ‚ùå **DISABLED**
- 10,000 file_complete events
- 100 progress updates (every 100 files) ‚úÖ **REDUCED**
- 1 stage_complete

---

## ‚öôÔ∏è Optimization Settings

### 1. Disable file_start Events ‚úÖ CRITICAL

**Why:** `file_start` events double your INSERTs but provide minimal value compared to `file_complete`

**File:** `cofi-service/.env`
```env
# Disable file_start logging for large batches
LOG_FILE_START_EVENTS=false
```

**Impact:**
- Reduces INSERTs by **50%** (from 168K ‚Üí 88K)
- Dashboard still shows real-time progress via `file_complete` events
- You can still see payloads (stored in `file_complete` events)

**Trade-off:**
- ‚ùå Can't see exact moment file processing starts
- ‚úÖ Still see when file completes (with full response)
- ‚úÖ Still see errors with payloads

---

### 2. Reduce Progress Update Frequency ‚úÖ IMPORTANT

**Why:** Progress updates every 10 files = 1,000 events per stage. Every 100 files = 100 events.

**File:** `cofi-service/.env`
```env
# Update progress every 100 files instead of 10
PROGRESS_UPDATE_INTERVAL=100
```

**Impact:**
- Reduces progress INSERTs by **90%** (from 1,000 ‚Üí 100 per stage)
- Dashboard still updates regularly (every 100 files)

**Options:**
- `10` - Small batches (< 1,000 files) - Very frequent updates
- `100` - Large batches (10,000 files) - ‚úÖ **RECOMMENDED**
- `250` - Very large batches (50,000+ files) - Minimal updates
- `500` - Huge batches (100,000+ files) - Maximum reduction

---

### 3. Increase Dashboard SSE Polling Interval ‚úÖ RECOMMENDED

**Why:** For large batches, you don't need sub-second updates

**File:** `cofi-dashboard/.env`
```env
# Poll database every 5 seconds instead of 2
SSE_POLL_INTERVAL=5
```

**Impact:**
- Reduces dashboard queries by **60%** (from 300 ‚Üí 120 per 10 minutes)
- Still provides timely updates (5-second delay)
- Less database load

**Options:**
- `2` seconds - Small batches (high priority)
- `5` seconds - Large batches (10,000+ files) - ‚úÖ **RECOMMENDED**
- `10` seconds - Historical monitoring only

---

## üéØ Recommended Configuration

### For 10,000-File Batches:

**File:** `cofi-service/.env`
```env
# Event Logging Optimization
LOG_FILE_START_EVENTS=false          # Save 50% INSERTs
PROGRESS_UPDATE_INTERVAL=100         # Update every 100 files
```

**File:** `cofi-dashboard/.env`
```env
# Dashboard Optimization
SSE_POLL_INTERVAL=5                  # Poll every 5 seconds
```

**Results:**
- ‚úÖ **48% fewer database writes** (168K ‚Üí 88K INSERTs)
- ‚úÖ **60% fewer dashboard queries**
- ‚úÖ Still get real-time progress updates
- ‚úÖ Still see full payloads/responses
- ‚úÖ Still track errors with details

---

## üìà Performance Metrics

### Before Optimization (10,000 files):

| Metric | Value |
|--------|-------|
| Total INSERTs | 168,000 |
| INSERTs per minute | ~3,000 |
| Dashboard queries per 10 min | 300 |
| Table growth per batch | ~100 MB |
| Event detail | Very high |

### After Optimization (10,000 files):

| Metric | Value |
|--------|-------|
| Total INSERTs | 88,000 |
| INSERTs per minute | ~1,600 |
| Dashboard queries per 10 min | 120 |
| Table growth per batch | ~50 MB |
| Event detail | High (still useful) |

---

## üîç What You Still Get

### Even with Optimizations:

‚úÖ **Real-time progress tracking**
- Stage start/complete events
- File complete events (every file)
- Progress checkpoints (every 100 files)

‚úÖ **Full payload/response data**
- Response data in `file_complete` events
- Payloads can be reconstructed from stage config
- Error payloads in `file_error` events

‚úÖ **Error tracking**
- All errors logged with full details
- Error messages captured
- Failed file tracking

‚úÖ **Dashboard functionality**
- Live progress bars
- File-by-file updates
- Clickable events for payload inspection
- All 10 stages monitored

---

## ‚ùå What You Lose (Minimal Impact)

### With file_start Disabled:

‚ùå **Exact start timestamp per file**
- Can't see exact moment processing began
- Only see completion time

‚ùå **Request payload in separate event**
- Payloads still available (in file_complete for errors)
- Just not logged for successful files

### Impact Assessment:
- **Low** - Most monitoring focuses on completion, not start
- **Acceptable** - Response data is more valuable than request
- **Recoverable** - Can re-enable if needed for debugging

---

## üóÑÔ∏è Database Storage Planning

### Storage Growth (per batch):

| Configuration | Storage per Batch | 100 Batches | 1000 Batches |
|---------------|-------------------|-------------|--------------|
| **Unoptimized** | ~100 MB | 10 GB | 100 GB |
| **Optimized** | ~50 MB | 5 GB | 50 GB |

### Recommendations:

#### 1. Regular Cleanup (IMPORTANT)
```sql
-- Delete events for batches older than 90 days
DELETE FROM batchExecutionLog
WHERE batchId IN (
    SELECT id FROM batchStatus
    WHERE createdAt < DATE_SUB(NOW(), INTERVAL 90 DAY)
);
```

#### 2. Archive Old Batches
```sql
-- Archive to separate table before deleting
CREATE TABLE batchExecutionLog_archive LIKE batchExecutionLog;

INSERT INTO batchExecutionLog_archive
SELECT * FROM batchExecutionLog
WHERE batchId IN (
    SELECT id FROM batchStatus
    WHERE createdAt < DATE_SUB(NOW(), INTERVAL 180 DAY)
);

-- Then delete from main table
DELETE FROM batchExecutionLog WHERE batchId IN (...);
```

#### 3. Partition Table by Date (Advanced)
```sql
-- Partition by month for automatic cleanup
ALTER TABLE batchExecutionLog
PARTITION BY RANGE (YEAR(timestamp) * 100 + MONTH(timestamp)) (
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    ...
);

-- Drop old partitions easily
ALTER TABLE batchExecutionLog DROP PARTITION p202401;
```

---

## üß™ Testing Your Configuration

### 1. Run a Test Batch

```bash
cd cofi-service
python -m src.main
```

### 2. Monitor Database Load

```bash
# Watch INSERT rate in real-time
watch -n 5 "mysql -u root -p -e 'SHOW GLOBAL STATUS LIKE \"Com_insert\"'"
```

### 3. Check Event Counts

```sql
-- Count events by type for latest batch
SELECT eventType, COUNT(*) as count
FROM batchExecutionLog
WHERE batchId = (SELECT MAX(id) FROM batchStatus)
GROUP BY eventType;
```

**Expected Results (Optimized):**
```
| eventType       | count  |
|-----------------|--------|
| stage_start     | 10     |
| file_complete   | 70,000 |  ‚Üê 10,000 files √ó 7 stages
| stage_progress  | 700    |  ‚Üê 100 updates √ó 7 stages
| stage_complete  | 10     |
| error           | < 100  |  ‚Üê Depends on failures
| TOTAL           | ~88,000|
```

**Without Optimization:**
```
| eventType       | count   |
|-----------------|---------|
| stage_start     | 10      |
| file_start      | 70,000  |  ‚Üê REMOVED in optimized
| file_complete   | 70,000  |
| stage_progress  | 7,000   |  ‚Üê 10x more than optimized
| stage_complete  | 10      |
| TOTAL           | ~168,000|
```

### 4. Verify Dashboard Performance

```bash
# Check dashboard query performance
mysql -u root -p testDb -e "
SELECT
    COUNT(*) as total_events,
    MAX(id) as max_id,
    MIN(id) as min_id
FROM batchExecutionLog
WHERE batchId = (SELECT MAX(id) FROM batchStatus);
"
```

---

## üöÄ Advanced Optimizations (Optional)

### For 50,000+ Files:

#### 1. Batch Insert Events
Instead of INSERT per event, batch multiple events:

```python
# In EventLogger, accumulate events
events_buffer = []

def file_complete(...):
    events_buffer.append(event_data)

    # Flush every 100 events
    if len(events_buffer) >= 100:
        repo.insert_many(events_buffer)
        events_buffer.clear()
```

**Impact:** Reduces INSERT overhead by 100x

#### 2. Async Logging
Log events asynchronously:

```python
import asyncio
from queue import Queue

event_queue = Queue()

async def event_worker():
    while True:
        event = await event_queue.get()
        repo.insert_event(**event)
```

**Impact:** Non-blocking event logging

#### 3. Use Redis for Live Events
```python
# Write to Redis for real-time
redis.publish('batch_events', json.dumps(event))

# Write to MySQL async for persistence
asyncio.create_task(db.insert_event(**event))
```

**Impact:** Zero blocking on database writes

---

## üìã Optimization Checklist

### Immediate Actions (5 minutes):
- [ ] Set `LOG_FILE_START_EVENTS=false` in cofi-service/.env
- [ ] Set `PROGRESS_UPDATE_INTERVAL=100` in cofi-service/.env
- [ ] Set `SSE_POLL_INTERVAL=5` in cofi-dashboard/.env
- [ ] Restart cofi-service
- [ ] Restart cofi-dashboard

### Monitoring (First batch):
- [ ] Watch database INSERT rate
- [ ] Verify dashboard updates are smooth
- [ ] Check table size growth
- [ ] Confirm progress tracking works

### Long-term (Monthly):
- [ ] Review batchExecutionLog table size
- [ ] Archive/delete old batch events (> 90 days)
- [ ] Monitor query performance
- [ ] Adjust settings if needed

---

## ‚úÖ Summary

### For 10,000-File Batches:

**Simple 2-Step Optimization:**

1. **cofi-service/.env:**
   ```env
   LOG_FILE_START_EVENTS=false
   PROGRESS_UPDATE_INTERVAL=100
   ```

2. **cofi-dashboard/.env:**
   ```env
   SSE_POLL_INTERVAL=5
   ```

**Results:**
- ‚úÖ 48% fewer database writes (168K ‚Üí 88K)
- ‚úÖ 60% fewer dashboard queries
- ‚úÖ Still fully functional real-time monitoring
- ‚úÖ Reduced table growth (100 MB ‚Üí 50 MB per batch)

**These settings are already configured in the .env.example files!**

Just copy them to your .env and you're good to go! üéâ
